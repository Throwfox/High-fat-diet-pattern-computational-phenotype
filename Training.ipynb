{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'processed_data.csv'\n",
    "df = pd.read_csv(data_path)        \n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling Preparation\n",
    "\n",
    "### Train/Test Split (80:20)\n",
    "Target Variables: `LFS`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "target_cols = ['LFS_10', 'LFS_15', 'LFS_20']\n",
    "drop_cols = target_cols\n",
    "drop_cols = [c for c in drop_cols if c in df.columns]\n",
    "\n",
    "X = df.drop(columns=drop_cols)\n",
    "y_10 = df['LFS_10']\n",
    "\n",
    "print(\"Feature set shape:\", X.shape)\n",
    "print(\"Target 28d distribution:\\n\", y_10.value_counts(dropna=False))\n",
    "\n",
    "# Split Data 80:20 (Stratified by 28-day mortality)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_10, test_size=0.2, random_state=42, stratify=y_10\n",
    ")\n",
    "\n",
    "print(f\"Training Set: {X_train.shape}\")\n",
    "print(f\"Testing Set:  {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score, accuracy_score, recall_score, roc_auc_score, \n",
    "    precision_recall_curve, classification_report\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE,RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "# 4. Calculate scale_pos_weight for XGBoost\n",
    "neg_count = (y_train == 0).sum()\n",
    "pos_count = (y_train == 1).sum()\n",
    "scale_pos_weight = neg_count / pos_count\n",
    "print(f\"\\nClass ratio: {neg_count}:{pos_count} = {scale_pos_weight:.1f}:1\")\n",
    "\n",
    "# 5. Define sampling strategies\n",
    "samplers = {\n",
    "    \"No Sampling\": None,\n",
    "    \"SMOTE\": SMOTE(random_state=42),\n",
    "    \"UnderSampler\": RandomUnderSampler(random_state=42),\n",
    "    \"RandomOverSampler\":RandomOverSampler(random_state=42)\n",
    "}\n",
    "\n",
    "# 6. Define models WITH class_weight / scale_pos_weight\n",
    "def get_models(scale_weight):\n",
    "    return {\n",
    "        \"LogReg\": LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "        \"RandomForest\": RandomForestClassifier(\n",
    "            n_estimators=100, random_state=42, class_weight='balanced'\n",
    "        ),\n",
    "        \"XGBoost\": XGBClassifier(\n",
    "            n_estimators=100, random_state=42, eval_metric=\"logloss\",\n",
    "            scale_pos_weight=scale_weight\n",
    "        )\n",
    "    }\n",
    "\n",
    "# 7. Function to find optimal threshold\n",
    "def find_best_threshold(y_true, y_proba):\n",
    "    \"\"\"Find threshold that maximizes F1\"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    return thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
    "\n",
    "# 8. Train and evaluate\n",
    "all_results = []\n",
    "\n",
    "for sampler_name, sampler in samplers.items():\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"Sampling: {sampler_name}\")\n",
    "    print(\"#\"*60)\n",
    "    \n",
    "    # Apply sampling\n",
    "    if sampler is not None:\n",
    "        X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "        print(f\"Resampled: {dict(pd.Series(y_train_res).value_counts())}\")\n",
    "    else:\n",
    "        X_train_res, y_train_res = X_train, y_train\n",
    "    \n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_res)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    models = get_models(scale_pos_weight)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n  {model_name}...\")\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            \"Sampling\": sampler_name,\n",
    "            \"Model\": model_name,\n",
    "            \"AUC\": roc_auc_score(y_test, y_proba),\n",
    "            \"F1\": f1_score(y_test, y_pred),\n",
    "            \"Recall\": recall_score(y_test, y_pred),\n",
    "            \"Specificity\": recall_score(y_test, y_pred, pos_label=0),\n",
    "            \"Precision\": (y_pred & y_test).sum() / y_pred.sum() if y_pred.sum() > 0 else 0\n",
    "        }\n",
    "        all_results.append(metrics)\n",
    "        print(f\"  AUC: {metrics['AUC']:.4f} | F1: {metrics['F1']:.4f} | Recall: {metrics['Recall']:.4f} | Specificity: {metrics['Specificity']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL SUMMARY (sorted by F1)\")\n",
    "print(\"=\"*60)\n",
    "results_df = pd.DataFrame(all_results).sort_values(\"F1\", ascending=False)\n",
    "print(results_df.round(4).to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BEST MODEL - Classification Report\")\n",
    "print(\"=\"*60)\n",
    "best = results_df.iloc[0]\n",
    "print(f\"Best: {best['Sampling']} + {best['Model']} (Threshold={best['Threshold']:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
